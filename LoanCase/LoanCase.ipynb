{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#Â Interpretability Case Study: Loan Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import statsmodels.api as sm\n",
    "import shap\n",
    "\n",
    "import Datasets as DS\n",
    "from ruxg import RUGClassifier\n",
    "from auxFunctions import get_leaves\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "The raw data and details for data cleaning are provided here: https://towardsdatascience.com/how-to-develop-a-credit-risk-model-and-scorecard-91335fc01f03 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data = DS.loan('../datasets/')\n",
    "print(loan_data.shape)\n",
    "loan_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data with stratification\n",
    "X = loan_data.drop('good_bad', axis = 1)\n",
    "y = loan_data[['good_bad']]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2,\n",
    "                                                    random_state = 42, stratify = y)\n",
    "\n",
    "X_train, X_test = X_train.copy(), X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 1\n",
    "\n",
    "We start with the objective of obtaining a high F1 score and, at the same time, providing interpretation for the results (if possible). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug = sm.add_constant(X_train)\n",
    "X_test_aug =  sm.add_constant(X_test)\n",
    "\n",
    "lr = sm.Logit(Y_train, X_train_aug).fit()\n",
    "\n",
    "threshold = 0.5\n",
    "Y_test['loan_pred'] = np.array(lr.predict(X_test_aug) > threshold , dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = pd.crosstab(Y_test['good_bad'], Y_test['loan_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "# Accuracies\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(Y_test['good_bad'], Y_test['loan_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(Y_test['good_bad'], Y_test['loan_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree (DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = DecisionTreeClassifier(criterion='gini', max_depth = 2, random_state=1)\n",
    "tc = tc.fit(X_train, Y_train)\n",
    "\n",
    "rule_length = get_leaves(tc, X_test)\n",
    "Y_test['loan_pred'] = tc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = pd.crosstab(Y_test['good_bad'], Y_test['loan_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "# Accuracies\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(Y_test['good_bad'], Y_test['loan_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(Y_test['good_bad'], Y_test['loan_pred']))\n",
    "print('Number of rules = %.0f' % tc.get_n_leaves())\n",
    "print('Average rule length = %.2f' % np.array(rule_length).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=7, random_state=1)\n",
    "rfc.fit(np.array(X_train), np.array(Y_train).flatten())\n",
    "\n",
    "Y_test['loan_pred'] = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(Y_test['good_bad'], Y_test['loan_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "n_leaves = 0\n",
    "for dtc in rfc.estimators_:\n",
    "    n_leaves += dtc.get_n_leaves()\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(Y_test['good_bad'], Y_test['loan_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(Y_test['good_bad'], Y_test['loan_pred']))\n",
    "print('Number of rules = %.0f' % n_leaves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost (ADA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(n_estimators = 30, random_state=1)\n",
    "ada.fit(X_train, Y_train)\n",
    "Y_test['loan_pred'] = ada.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(Y_test['good_bad'], Y_test['loan_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "# n_leaves = sum(tree.tree_.n_leaves for tree in gbc.estimators_.reshape(-1))\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(Y_test['good_bad'], Y_test['loan_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(Y_test['good_bad'], Y_test['loan_pred']))\n",
    "print('Number of rules = %.0f' % (2*ada.n_estimators))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting (GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(max_depth = 2, n_estimators = 30, random_state=1)\n",
    "gbc.fit(X_train, Y_train)\n",
    "Y_test['loan_pred'] = gbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(Y_test['good_bad'], Y_test['loan_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "n_leaves = sum(tree.tree_.n_leaves for tree in gbc.estimators_.reshape(-1))\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(Y_test['good_bad'], Y_test['loan_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(Y_test['good_bad'], Y_test['loan_pred']))\n",
    "print('Number of rules = %.0f' % n_leaves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUG\n",
    "\n",
    "To obtain a simple and interpretable model with a better F1 score, we tune the RUG model. We choose the maximum depth as two, and maximum number of rule generations as three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnp = np.array(X_train)\n",
    "Ynp = np.array(Y_train).flatten()\n",
    "\n",
    "RUG = RUGClassifier(max_depth=2, max_RMP_calls=3,\n",
    "                    rule_length_cost=True, solver='gurobi', random_state=1)\n",
    "RUG_fit = RUG.fit(Xnp, Ynp)\n",
    "\n",
    "Y_test['loan_pred'] = RUG.predict(np.array(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(Y_test['good_bad'], Y_test['loan_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(Y_test['good_bad'], Y_test['loan_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(Y_test['good_bad'], Y_test['loan_pred']))\n",
    "print('Number of rules = %.0f' % RUG.get_num_of_rules())\n",
    "print('Average rule length = %.2f' % RUG.get_avg_rule_length())\n",
    "\n",
    "dict1, dict2 = RUG.get_instance_to_rule_dicts(X_test.index, X_test)\n",
    "print('Average number of rules used per sample: %.2f' % np.mean(list(dict2.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print rules generated by RUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUG.print_rules(feature_names=X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with SHAP applied to GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us explain the predictions of GB model with SHAP and then compare it against RUG's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(gbc, X_test)\n",
    "shap_values = explainer(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 6\n",
    "id = X_test.index[i]\n",
    "\n",
    "# rules covering that instance\n",
    "RUG.print_rules_for_instances([id], dict1, colnames=X_test.columns)\n",
    "\n",
    "# shap\n",
    "shap.plots.waterfall(shap_values[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 18\n",
    "id = X_test.index[i]\n",
    "\n",
    "# rules covering that instance\n",
    "RUG.print_rules_for_instances([id], dict1, colnames=X_test.columns)\n",
    "\n",
    "# shap\n",
    "shap.plots.waterfall(shap_values[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 2\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "see above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree (DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgrid = {'max_depth': [3,5,7,9,11,13,15]}\n",
    "\n",
    "tc_estimator = DecisionTreeClassifier(criterion='gini', random_state=1)\n",
    "gcv = GridSearchCV(estimator=tc_estimator, param_grid=pgrid, n_jobs=1, cv=5, verbose=0, refit=True)\n",
    "gcv_fit = gcv.fit(X_train, Y_train)\n",
    "tc = gcv_fit.best_estimator_\n",
    "\n",
    "Y_test['loan_pred'] = tcc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv_fit.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = pd.crosstab(Y_test['good_bad'], Y_test['loan_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "# Accuracies\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(Y_test['good_bad'], Y_test['loan_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(Y_test['good_bad'], Y_test['loan_pred']))\n",
    "print('Number of rules = %.0f' % tc.get_n_leaves())\n",
    "print('Average rule length = %.2f' % np.array(rule_length).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgrid = {'max_depth': [3,5,7,9,11,13,15], \n",
    "         'n_estimators':[100, 150, 200,250,300]}\n",
    "\n",
    "rf_estimator = RandomForestClassifier(criterion='gini', random_state=1)\n",
    "gcv = GridSearchCV(estimator=rf_estimator, param_grid=pgrid, n_jobs=1, cv=5, verbose=0, refit=True)\n",
    "gcv_fit = gcv.fit(X_train, Y_train)\n",
    "rfc = gcv_fit.best_estimator_\n",
    "\n",
    "Y_test['loan_pred'] = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv_fit.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(Y_test['good_bad'], Y_test['loan_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "n_leaves = 0\n",
    "for dtc in rfc.estimators_:\n",
    "    n_leaves += dtc.get_n_leaves()\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(Y_test['good_bad'], Y_test['loan_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(Y_test['good_bad'], Y_test['loan_pred']))\n",
    "print('Number of rules = %.0f' % n_leaves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgrid = {'n_estimators':[100,150,200,250,30]}\n",
    "\n",
    "ada_estimator = AdaBoostClassifier(random_state=1)    \n",
    "gcv = GridSearchCV(estimator=ada_estimator, param_grid=pgrid, n_jobs=1, cv=5, verbose=0, refit=True)\n",
    "gcv_fit = gcv.fit(X_train, Y_train)\n",
    "ada = gcv_fit.best_estimator_\n",
    "\n",
    "Y_test['loan_pred'] = ada.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv_fit.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(Y_test['good_bad'], Y_test['loan_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "# n_leaves = sum(tree.tree_.n_leaves for tree in gbc.estimators_.reshape(-1))\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(Y_test['good_bad'], Y_test['loan_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(Y_test['good_bad'], Y_test['loan_pred']))\n",
    "print('Number of rules = %.0f' % (2*ada.n_estimators))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgrid = {'max_depth':[3,5,7,9,11,15],\n",
    "         'n_estimators':[100,150,200,250,30]}\n",
    "\n",
    "gb_estimator = GradientBoostingClassifier(random_state=1)\n",
    "gcv = GridSearchCV(estimator=gb_estimator, param_grid=pgrid, n_jobs=1, cv=5, verbose=0, refit=True)\n",
    "gcv_fit = gcv.fit(X_train, Y_train)\n",
    "gbc = gcv_fit.best_estimator_\n",
    "\n",
    "Y_test['loan_pred'] = gbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv_fit.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(Y_test['good_bad'], Y_test['loan_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "n_leaves = sum(tree.tree_.n_leaves for tree in gbc.estimators_.reshape(-1))\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(Y_test['good_bad'], Y_test['loan_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(Y_test['good_bad'], Y_test['loan_pred']))\n",
    "print('Number of rules = %.0f' % n_leaves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnp = np.array(X_train)\n",
    "Ynp = np.array(Y_train).flatten()\n",
    "\n",
    "RUG = RUGClassifier(max_depth=2, rule_length_cost=True, \n",
    "                    solver='gurobi', random_state=21)\n",
    "RUG_fit = RUG.fit(Xnp, Ynp)\n",
    "\n",
    "Y_predict = RUG.predict(np.array(X_test))\n",
    "Y_test['loan_pred'] = Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(Y_test['good_bad'], Y_test['loan_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(Y_test['good_bad'], Y_test['loan_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(Y_test['good_bad'], Y_test['loan_pred']))\n",
    "print('Number of rules = %.0f' % RUG.get_num_of_rules())\n",
    "print('Average rule length = %.2f' % RUG.get_avg_rule_length())\n",
    "\n",
    "dict1, dict2 = RUG.get_instance_to_rule_dicts(X_test.index, X_test)\n",
    "print('Average number of rules used per sample: %.2f' % np.mean(list(dict2.values())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
