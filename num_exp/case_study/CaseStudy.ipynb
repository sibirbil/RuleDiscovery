{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the paper we present the case study described in Section 4.3 of the manuscript. Specifically, we compare the results of our method RUG with those of FSDT (McTavish et al., 2021) and CG (Lawless et al., 2021).\n",
    "\n",
    "The results of this analysis are reported in Table 5 of the manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, matthews_corrcoef\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from lightgbm import LGBMClassifier\n",
    "import statsmodels.api as sm\n",
    "import shap\n",
    "\n",
    "# import Datasets as DS\n",
    "import grid_search_helpers as gs_helpers\n",
    "from case_study_helpers import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# for RUG\n",
    "from ruxg import RUGClassifier\n",
    "\n",
    "# for FSDT\n",
    "import FSDT_helpers as FSDT_helpers\n",
    "from dl85 import DL85Classifier\n",
    "\n",
    "# for CG\n",
    "from CG_helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "The raw data and details for data cleaning are provided here: https://towardsdatascience.com/how-to-develop-a-credit-risk-model-and-scorecard-91335fc01f03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# problem = DS.loan\n",
    "# pname = problem.__name__.upper()\n",
    "pname = 'LOAN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Table 5a\n",
    "\n",
    "We start with the objective of obtaining an interpretable model. We will then examine the performance of these interpretable model(s) genereated by different approaches.\n",
    "\n",
    "To ensure the models are interpretable, we set the maximum depth to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "depth = 2\n",
    "y_results = pd.DataFrame() # dataframe to save the predicted values of each method\n",
    "\n",
    "# some parameters for reproducibility\n",
    "random_state = 42\n",
    "test_size = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prep the data\n",
    "import Datasets as DS\n",
    "X_train, X_test, y_train, y_test = gs_helpers.prep_data(DS.loan, binary=False, randomState=random_state,\n",
    "                                                        testSize=test_size, target='good_bad')\n",
    "\n",
    "# initialize classifier\n",
    "RUG = RUGClassifier(max_depth=depth, rule_length_cost=True,\n",
    "                    solver='gurobi', random_state=1, max_RMP_calls=3)\n",
    "\n",
    "print('Fitting RUG')\n",
    "RUG_fit = RUG.fit(X_train, y_train)\n",
    "\n",
    "# get predited values\n",
    "y_results['RUG_pred'] = RUG.predict(np.array(X_test))\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(y_test, y_results['RUG_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(y_test, y_results['RUG_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(y_test, y_results['RUG_pred']))\n",
    "print('MCC score  = %.4f' % matthews_corrcoef(y_test, y_results['RUG_pred']))\n",
    "print('Number of rules = %.0f' % RUG.get_num_of_rules())\n",
    "print('Average rule length = %.2f' % RUG.get_avg_rule_length())\n",
    "print('Average number of rules used per sample = %.2f' % RUG.get_avg_num_rules_per_sample())\n",
    "print('Average number of rules used per sample = %.2f' % RUG.get_avg_rule_length_per_sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FSDT\n",
    "\n",
    "McTavish, H., Zhong, C., Achermann, R., Karimalis, I., Chen, J., Rudin, C., & Seltzer, M. (2022). Fast Sparse Decision Tree Optimization via Reference Ensembles. Proceedings of the AAAI Conference on Artificial Intelligence, 36(9), 9604-9613. https://doi.org/10.1609/aaai.v36i9.21194"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import Datasets_binary as DS\n",
    "X_train, X_test, y_train, y_test = gs_helpers.prep_data(DS.loan, binary=False, randomState=random_state, testSize=test_size, target='y', datasets_path='./FSDT_binarized/')\n",
    "\n",
    "# initialize classifier\n",
    "FSDT = DL85Classifier(time_limit=1000, desc=True, max_depth = depth)\n",
    "\n",
    "print('Fitting FSDT')\n",
    "FSDT.fit(X_train, y_train)\n",
    "\n",
    "# get predited values\n",
    "y_results['FSDT_pred'] = FSDT.predict(X_test)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(y_test, y_results['FSDT_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(y_test, y_results['FSDT_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(y_test, y_results['FSDT_pred']))\n",
    "print('MCC score  = %.4f' % matthews_corrcoef(y_test, y_results['FSDT_pred']))\n",
    "print('Number of rules = %.0f' % FSDT_helpers.get_num_leaves(FSDT.tree_))\n",
    "print('Average rule length = %.2f' % FSDT_helpers.get_avg_rule_length(FSDT.tree_))\n",
    "print('Average number of rules used per sample = %.2f' % 1.00)\n",
    "print('Average number of rules used per sample = %.2f' % FSDT_helpers.get_avg_rule_length_per_sample(FSDT.tree_, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Column Generation (CG)\n",
    "\n",
    "Lawless, C., Dash, S., G端nl端k, O., & Wei, D. (2021). Interpretable and Fair Boolean Rule Sets via Column Generation. arXiv preprint arXiv:2111.08466.\n",
    "\n",
    "This method does not have a maximum depth parameter. Instead, we are specifying a parameter to control sparsity, which they refer to as complexity. This parameter does not directly translate to the depth of the rules. We refer to their paper for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "CG_pgrid = {'epsilon':1,\n",
    "            'complexity':5}\n",
    "\n",
    "import Datasets_binary as DS\n",
    "X_train, X_test, y_train, y_test = gs_helpers.prep_data(DS.loan, binary=False, randomState=random_state, testSize=test_size, target='y', datasets_path='./datasets/CG_binarized/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print('Fitting CG')\n",
    "res, classif = run_CG(pname, X_train, X_test, y_train, y_test, CG_pgrid, fairness_metric='unfair', time_limit=1000)\n",
    "final_rule_set = classif.fitRuleSet\n",
    "\n",
    "# get predicted values\n",
    "y_results['CG_pred'] = classif.predict(X_test)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(y_test, y_results['CG_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(y_test, y_results['CG_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(y_test, y_results['CG_pred']))\n",
    "print('MCC score  = %.4f' % matthews_corrcoef(y_test, y_results['CG_pred']))\n",
    "print('Number of rules = %.0f' % len(final_rule_set))\n",
    "print('Average rule length = %.2f' % np.mean(np.sum(final_rule_set, axis=1)))\n",
    "nr_rules_sample, length_sample = CG_rules_per_sample(X_test, final_rule_set)\n",
    "print('Average number of rules used per sample = %.2f' % nr_rules_sample)\n",
    "print('Average number of rules used per sample = %.2f' % length_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Table 5b\n",
    "\n",
    "For this part, we try to increase the performance of the model and then examine their level of interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import Datasets as DS\n",
    "X_train, X_test, y_train, y_test = gs_helpers.prep_data(DS.loan, binary=False, randomState=random_state, testSize=test_size, target='good_bad')\n",
    "\n",
    "# initialize model\n",
    "RUG = RUGClassifier(max_depth=2, rule_length_cost=True,\n",
    "                    solver='gurobi', random_state=0)\n",
    "\n",
    "print('Fitting RUG')\n",
    "RUG_fit = RUG.fit(X_train, y_train)\n",
    "\n",
    "# get predicted values\n",
    "y_results['RUG_pred'] = RUG.predict(np.array(X_test))\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(y_test, y_results['RUG_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(y_test, y_results['RUG_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(y_test, y_results['RUG_pred']))\n",
    "print('MCC score  = %.4f' % matthews_corrcoef(y_test, y_results['RUG_pred']))\n",
    "print('Number of rules = %.0f' % RUG.get_num_of_rules())\n",
    "print('Average rule length = %.2f' % RUG.get_avg_rule_length())\n",
    "print('Average number of rules used per sample = %.2f' % RUG.get_avg_num_rules_per_sample())\n",
    "print('Average number of rules used per sample = %.2f' % RUG.get_avg_rule_length_per_sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FSDT\n",
    "\n",
    "McTavish, H., Zhong, C., Achermann, R., Karimalis, I., Chen, J., Rudin, C., & Seltzer, M. (2022). Fast Sparse Decision Tree Optimization via Reference Ensembles. Proceedings of the AAAI Conference on Artificial Intelligence, 36(9), 9604-9613. https://doi.org/10.1609/aaai.v36i9.21194"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import Datasets_binary as DS\n",
    "X_train, X_test, y_train, y_test = gs_helpers.prep_data(DS.loan, binary=False, randomState=random_state,testSize=test_size, target='y', datasets_path='./datasets/FSDT_binarized/')\n",
    "\n",
    "# initialize classifier\n",
    "FSDT = DL85Classifier(time_limit=1000, desc=True, max_depth = 3)\n",
    "\n",
    "print('Fitting FSDT')\n",
    "FSDT.fit(X_train, y_train)\n",
    "\n",
    "# get predited values\n",
    "y_results['FSDT_pred'] = FSDT.predict(X_test)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(y_test, y_results['FSDT_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(y_test, y_results['FSDT_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(y_test, y_results['FSDT_pred']))\n",
    "print('MCC score  = %.4f' % matthews_corrcoef(y_test, y_results['FSDT_pred']))\n",
    "print('Number of rules = %.0f' % FSDT_helpers.get_num_leaves(FSDT.tree_))\n",
    "print('Average rule length = %.2f' % FSDT_helpers.get_avg_rule_length(FSDT.tree_))\n",
    "print('Average number of rules used per sample = %.2f' % 1.00)\n",
    "print('Average number of rules used per sample = %.2f' % FSDT_helpers.get_avg_rule_length_per_sample(FSDT.tree_, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Column Generation (CG)\n",
    "\n",
    "Lawless, C., Dash, S., G端nl端k, O., & Wei, D. (2021). Interpretable and Fair Boolean Rule Sets via Column Generation. arXiv preprint arXiv:2111.08466.\n",
    "\n",
    "This method does not have a maximum depth parameter. Instead, we are specifying a parameter to control sparsity, which they refer to as complexity. This parameter does not directly translate to the depth of the rules. We refer to their paper for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "CG_pgrid = {'epsilon':1,\n",
    "            'complexity':25}\n",
    "\n",
    "import Datasets_binary as DS\n",
    "X_train, X_test, y_train, y_test = gs_helpers.prep_data(DS.loan, binary=False, randomState=random_state,testSize=test_size, target='y', datasets_path='./datasets/CG_binarized/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print('Fitting CG')\n",
    "res, classif = run_CG(pname, X_train, X_test, y_train, y_test, CG_pgrid, fairness_metric='unfair', time_limit=1000)\n",
    "final_rule_set = classif.fitRuleSet\n",
    "\n",
    "# get predicted values\n",
    "y_results['CG_pred'] = classif.predict(X_test)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(y_test, y_results['CG_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(y_test, y_results['CG_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(y_test, y_results['CG_pred']))\n",
    "print('MCC score  = %.4f' % matthews_corrcoef(y_test, y_results['CG_pred']))\n",
    "print('Number of rules = %.0f' % len(final_rule_set))\n",
    "print('Average rule length = %.2f' % np.mean(np.sum(final_rule_set, axis=1)))\n",
    "nr_rules_sample, length_sample = CG_rules_per_sample(X_test, final_rule_set)\n",
    "print('Average number of rules used per sample = %.2f' % nr_rules_sample)\n",
    "print('Average number of rules used per sample = %.2f' % length_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## SHAP Figures\n",
    "\n",
    "We further compare the rules generated by RUG with the feature importance values generated by SHAP (Lundberg & Lee, 2017) applied to LightGBM. Hence, we'll first train the LightGBM model. Then, we'll select two samples and inspect the rules and the SHAP values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### RUG\n",
    "\n",
    "For this analysis, we will use the RUG model from Table 5a, i.e. the RUG model with 7 rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import Datasets as DS\n",
    "# prep the data\n",
    "X_train, X_test, y_train, y_test = gs_helpers.prep_data(DS.loan, binary=False, randomState=random_state,\n",
    "                                                        testSize=test_size, target='good_bad')\n",
    "\n",
    "# initialize classifier\n",
    "RUG = RUGClassifier(max_depth=depth, rule_length_cost=True,\n",
    "                    solver='gurobi', random_state=1, max_RMP_calls=3)\n",
    "\n",
    "print('Fitting RUG')\n",
    "RUG_fit = RUG.fit(X_train, y_train)\n",
    "\n",
    "# get predited values\n",
    "y_results['RUG_pred'] = RUG.predict(np.array(X_test))\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "colnames = DS.loan('./datasets/').columns.tolist()\n",
    "colnames.remove('good_bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = gs_helpers.prep_data(DS.loan, binary=False, randomState=random_state,\n",
    "                                                        testSize=test_size, target='good_bad')\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns = colnames)\n",
    "X_test = pd.DataFrame(X_test, columns = colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "depth = 2\n",
    "\n",
    "# initialize classifier\n",
    "lgbm = LGBMClassifier(objective='binary', random_state=0, max_depth=depth, num_leaves=(2**depth), n_estimators=20)\n",
    "\n",
    "print('Fitting LightGBM')\n",
    "lgbm.fit(X_train, y_train)\n",
    "\n",
    "# get predicted values\n",
    "y_results['LightGBM_pred'] = lgbm.predict(X_test)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(lgbm, X_test)\n",
    "shap_values = explainer(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Compare SHAP output with rules\n",
    "\n",
    "We compare the output of SHAP applied to LightGBM with the rules generated by RUG. We do this for two arbitrarily chosen samples from the test set, where one of the samples is covered by two rules and the other one by just one rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dict1, dict2 = RUG.get_instance_to_rule_dicts(pd.DataFrame(X_test).index, pd.DataFrame(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "i = 18\n",
    "# rules covering that instance\n",
    "print(f'Instance {i}')\n",
    "RUG.print_rules_for_instances([i], dict1)\n",
    "\n",
    "# shap\n",
    "shap.plots.waterfall(shap_values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "i = 62\n",
    "# rules covering that instance\n",
    "print(f'Instance {i}')\n",
    "RUG.print_rules_for_instances([i], dict1)\n",
    "\n",
    "# shap\n",
    "shap.plots.waterfall(shap_values[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "# Appendix\n",
    "\n",
    "In this part of the paper we repeat the case study described above (see Section 4.3 in the manuscript) with traditional machine learning models, including logistic regression (LR), decision tree (DT), random forest (RF), AdaBoost (ADA), and LightGBM.\n",
    "\n",
    "The results of this analysis are reported in Table 8 and described in Appendix D in the manuscript."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Table 8a\n",
    "\n",
    "We start with the objective of obtaining an interpretable model. We will then examine the performance of these interpretable model(s) genereated by different approaches.\n",
    "\n",
    "To ensure the models are interpretable, we set the maximum depth to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import Datasets as DS\n",
    "X_train, X_test, y_train, y_test = gs_helpers.prep_data(DS.loan, binary=False, randomState=random_state,\n",
    "                                                        testSize=test_size, target='good_bad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train_aug = sm.add_constant(X_train)\n",
    "X_test_aug =  sm.add_constant(X_test)\n",
    "\n",
    "lr = sm.Logit(y_train, X_train_aug).fit()\n",
    "\n",
    "threshold = 0.5\n",
    "y_results['LR_pred'] = np.array(lr.predict(X_test_aug) > threshold , dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = pd.crosstab(y_test, y_results['LR_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "# Accuracies\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(y_test, y_results['LR_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(y_test, y_results['LR_pred']))\n",
    "print('MCC score  = %.4f' % matthews_corrcoef(y_test, y_results['LR_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree (DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tc = DecisionTreeClassifier(criterion='gini', max_depth = 2, random_state=1)\n",
    "tc = tc.fit(X_train, y_train)\n",
    "\n",
    "y_results['DT_pred'] = tc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = pd.crosstab(y_test, y_results['DT_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "# Accuracies\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(y_test, y_results['DT_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(y_test, y_results['DT_pred']))\n",
    "print('MCC score  = %.4f' % matthews_corrcoef(y_test, y_results['DT_pred']))\n",
    "print('Number of rules = %.0f' % tc.get_n_leaves())\n",
    "print('Average rule length = %.2f' % average_depth(tc))\n",
    "print('Average number of rules per sample = %.2f' % 1)\n",
    "print('Average rule length per sample = %.2f' % average_path_length(tc, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=20, criterion='gini', max_depth=7, random_state=1)\n",
    "rfc.fit(np.array(X_train), np.array(y_train).flatten())\n",
    "\n",
    "y_results['RF_pred'] = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(y_test, y_results['RF_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "n_leaves = 0\n",
    "for dtc in rfc.estimators_:\n",
    "    n_leaves += dtc.get_n_leaves()\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(y_test, y_results['RF_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(y_test, y_results['RF_pred']))\n",
    "print('MCC score  = %.4f' % matthews_corrcoef(y_test, y_results['RF_pred']))\n",
    "print('Number of rules = %.0f' % n_leaves)\n",
    "print('Average rule length = %.2f' % avg_depth_ensemble(rfc))\n",
    "print('Average rule length per sample = %.2f' % avg_path_length_ensemble(rfc, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost (ADA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(n_estimators = 20, random_state=1)\n",
    "ada.fit(X_train, y_train)\n",
    "y_results['ADA_pred'] = ada.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(y_test, y_results['ADA_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(y_test, y_results['ADA_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(y_test, y_results['ADA_pred']))\n",
    "print('MCC score  = %.4f' % matthews_corrcoef(y_test, y_results['ADA_pred']))\n",
    "print('Number of rules = %.0f' % (2*ada.n_estimators))\n",
    "print('Number of rules per sample = %.0f' % ada.n_estimators)\n",
    "print('Average rule length = %.2f' % avg_depth_ensemble(ada))\n",
    "print('Average rule length per sample = %.2f' % avg_path_length_ensemble(ada, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lgbm = LGBMClassifier(objective='binary', random_state=0, max_depth=2, num_leaves=(2**depth), n_estimators=20)\n",
    "lgbm.fit(X_train, y_train)\n",
    "y_results['LightGBM_pred'] = lgbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(y_test, y_results['LightGBM_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "n_leaves = sum(tree['num_leaves'] for tree in lgbm._Booster.dump_model()[\"tree_info\"])\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(y_test, y_results['LightGBM_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(y_test, y_results['LightGBM_pred']))\n",
    "print('MCC score  = %.4f' % matthews_corrcoef(y_test, y_results['LightGBM_pred']))\n",
    "print('Number of rules = %.0f' % n_leaves)\n",
    "print('Average rule length = %.2f' % lgbm.get_params()['max_depth'])\n",
    "print('Average number of rules per sample = %.2f' % lgbm.get_params()['n_estimators'])\n",
    "print('Average rule length per sample = %.2f' % lgbm.get_params()['max_depth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Table 8b\n",
    "\n",
    "For this part, we try to increase the performance of the model and then examine their level of interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import Datasets as DS\n",
    "X_train, X_test, y_train, y_test = gs_helpers.prep_data(DS.loan, binary=False, randomState=random_state,\n",
    "                                                        testSize=test_size, target='good_bad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (LR)\n",
    "\n",
    "see above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree (DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pgrid = {'max_depth': [3,5,7,9,11,13,15]}\n",
    "\n",
    "tc_estimator = DecisionTreeClassifier(criterion='gini', random_state=1)\n",
    "gcv = GridSearchCV(estimator=tc_estimator, param_grid=pgrid, n_jobs=1, cv=5, verbose=0, refit=True)\n",
    "gcv_fit = gcv.fit(X_train, y_train)\n",
    "tc = gcv_fit.best_estimator_\n",
    "\n",
    "y_results['DT_pred'] = tc.predict(X_test)\n",
    "\n",
    "gcv_fit.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = pd.crosstab(y_test, y_results['DT_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "# Accuracies\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(y_test, y_results['DT_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(y_test, y_results['DT_pred']))\n",
    "print('MCC score  = %.4f' % matthews_corrcoef(y_test, y_results['DT_pred']))\n",
    "print('Number of rules = %.0f' % tc.get_n_leaves())\n",
    "print('Average rule length = %.2f' % average_depth(tc))\n",
    "print('Average rule length per sample = %.2f' % average_path_length(tc, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pgrid = {'max_depth': [3,5,7,9,11,13,15],\n",
    "         'n_estimators':[100, 150, 200,250,300]}\n",
    "\n",
    "rf_estimator = RandomForestClassifier(criterion='gini', random_state=1)\n",
    "gcv = GridSearchCV(estimator=rf_estimator, param_grid=pgrid, n_jobs=1, cv=5, verbose=1, refit=True)\n",
    "gcv_fit = gcv.fit(X_train, y_train)\n",
    "rfc = gcv_fit.best_estimator_\n",
    "\n",
    "y_results['RF_pred'] = rfc.predict(X_test)\n",
    "\n",
    "gcv_fit.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(y_test, y_results['RF_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "n_leaves = 0\n",
    "for dtc in rfc.estimators_:\n",
    "    n_leaves += dtc.get_n_leaves()\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(y_test, y_results['RF_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(y_test, y_results['RF_pred']))\n",
    "print('MCC score  = %.4f' % matthews_corrcoef(y_test, y_results['RF_pred']))\n",
    "print('Number of rules = %.0f' % n_leaves)\n",
    "print('Average rule length = %.2f' % avg_depth_ensemble(rfc))\n",
    "print('Average rule length per sample = %.2f' % avg_path_length_ensemble(rfc, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost (ADA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pgrid = {'n_estimators':[100,150,200,250,300]}\n",
    "\n",
    "ada_estimator = AdaBoostClassifier(random_state=1)\n",
    "gcv = GridSearchCV(estimator=ada_estimator, param_grid=pgrid, n_jobs=1, cv=5, verbose=1, refit=True)\n",
    "gcv_fit = gcv.fit(X_train, y_train)\n",
    "ada = gcv_fit.best_estimator_\n",
    "\n",
    "y_results['ADA_pred'] = ada.predict(X_test)\n",
    "\n",
    "gcv_fit.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(y_test, y_results['ADA_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "# n_leaves = sum(tree.tree_.n_leaves for tree in gbc.estimators_.reshape(-1))\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(y_test, y_results['ADA_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(y_test, y_results['ADA_pred']))\n",
    "print('MCC score  = %.4f' % matthews_corrcoef(y_test, y_results['ADA_pred']))\n",
    "print('Number of rules = %.0f' % (2*ada.n_estimators))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pgrid = {'max_depth':[3,5,7,9,11,15],\n",
    "         'n_estimators':[100,150,200,250,300]}\n",
    "\n",
    "lgbm_estimator = LGBMClassifier(random_state=1)\n",
    "gcv = GridSearchCV(estimator=lgbm_estimator, param_grid=pgrid, n_jobs=1, cv=5, verbose=1, refit=True)\n",
    "gcv_fit = gcv.fit(X_train, y_train)\n",
    "gbc = gcv_fit.best_estimator_\n",
    "\n",
    "y_results['LightGBM_pred'] = gbc.predict(X_test)\n",
    "\n",
    "gcv_fit.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(y_test, y_results['LightGBM_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "n_leaves = sum(tree['num_leaves'] for tree in gcv_fit.best_estimator_._Booster.dump_model()[\"tree_info\"])\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(y_test, y_results['LightGBM_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(y_test, y_results['LightGBM_pred']))\n",
    "print('MCC score  = %.4f' % matthews_corrcoef(y_test, y_results['LightGBM_pred']))\n",
    "print('Number of rules = %.0f' % n_leaves)\n",
    "print('Average rule length = %.2f' % gcv_fit.best_estimator_.get_params()['max_depth'])\n",
    "print('Average number of rules per sample = %.2f' % gcv_fit.best_estimator_.get_params()['n_estimators'])\n",
    "print('Average rule length per sample = %.2f' % gcv_fit.best_estimator_.get_params()['max_depth'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}